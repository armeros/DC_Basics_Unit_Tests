{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unit_Test",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5gn50Ogepjz",
        "outputId": "23120566-a156-42f9-ff6c-59dbc73b8bde"
      },
      "source": [
        "# เขียนเอง ไม่รู้วิธีดูต้นฉบับ\n",
        "\n",
        "def row_to_list(values):\n",
        "    if \"\\t\" not in values:\n",
        "        return None\n",
        "    else:\n",
        "        return values.replace(\"\\n\",'').replace(\"\\t\", \" \").split(\" \")\n",
        "\n",
        "# เขียนมั่ว ไม่ใช่จุดประสงค์\n",
        "\n",
        "def row_to_list_bugfix(values):\n",
        "    if (\"\\t\" not in values) | (values[0] == \"\\t\"):\n",
        "        return None\n",
        "    else:\n",
        "        return values.replace(\"\\n\",'').replace(\"\\t\", \" \").split(\" \")\n",
        "\n",
        "print(row_to_list(\"2,081\\t314,942\\n\"))\n",
        "print(row_to_list(\"\\t293,410\\n\"))\n",
        "print(row_to_list(\"1,463238,765\\n\"))\n",
        "\n",
        "print(row_to_list_bugfix(\"2,081\\t314,942\\n\"))\n",
        "print(row_to_list_bugfix(\"\\t293,410\\n\"))\n",
        "print(row_to_list_bugfix(\"1,463238,765\\n\"))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2,081', '314,942']\n",
            "['', '293,410']\n",
            "None\n",
            "['2,081', '314,942']\n",
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egt2rb96gyHz",
        "outputId": "181dbeba-46a6-4da6-aa88-4231e55edcae"
      },
      "source": [
        "# Import the pytest package\n",
        "import pytest\n",
        "\n",
        "# Import the function convert_to_int()\n",
        "from preprocessing_helpers import convert_to_int\n",
        "\n",
        "# Complete the unit test name by adding a prefix\n",
        "def test_on_string_with_one_comma():\n",
        "    # Complete the assert statement\n",
        "    assert convert_to_int(\"2,081\") == 2081\n",
        "\n",
        "!pytest test_convert_to_int.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_convert_to_int.py F\u001b[36m                                                 [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m________________________ test_on_string_with_one_comma _________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_on_string_with_one_comma():\u001b[0m\n",
            "\u001b[1m        # Complete the assert statement\u001b[0m\n",
            "\u001b[1m>       assert convert_to_int(\"2,081\") == \"2081\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AssertionError: assert 2081 == '2081'\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where 2081 = convert_to_int('2,081')\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:11: AssertionError\n",
            "\u001b[1m\u001b[31m=========================== 1 failed in 0.04 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ7DGAdmg9LZ",
        "outputId": "a4203a2b-f73e-437d-c4b5-e4a3ebb70622"
      },
      "source": [
        "# ถ้าผ่าน จะได้ผลลัพธ์หน้าตาแบบนี้\n",
        "\n",
        "!pytest test_convert_to_int.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_convert_to_int.py .\u001b[36m                                                 [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.02 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ciRPt-NmTV"
      },
      "source": [
        "def convert_to_int(string_with_comma):\n",
        "    # Fix this line so that it returns an int, not a str\n",
        "    return int(string_with_comma.replace(\",\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg7ZXo_FTzbG"
      },
      "source": [
        "import numpy as np\n",
        "import pytest\n",
        "\n",
        "from mystery_function import mystery_function\n",
        "\n",
        "def test_on_clean_data():\n",
        "    assert np.array_equal(mystery_function(\"example_clean_data.txt\", num_columns=2), np.array([[2081.0, 314942.0], [1059.0, 186606.0]]))\n",
        "\n",
        "# It converts data in a data file into a NumPy array.\n",
        "# it is a good idea to tell them to look at the unit tests if they are not sure about a function's purpose.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tFLdw-PUnL2",
        "outputId": "6596fa56-642c-4763-800e-feeb422cce4b"
      },
      "source": [
        "# Write better assert statement\n",
        "\n",
        "import pytest\n",
        "from preprocessing_helpers import convert_to_int\n",
        "\n",
        "def test_on_string_with_one_comma():\n",
        "    test_argument = \"2,081\"\n",
        "    expected = 2081\n",
        "    actual = convert_to_int(test_argument)\n",
        "    # Format the string with the actual return value\n",
        "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
        "    # Write the assert statement which prints message on failure\n",
        "    assert actual == expected, message\n",
        "\n",
        "!pytest test_convert_to_int.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_convert_to_int.py F\u001b[36m                                                 [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m________________________ test_on_string_with_one_comma _________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_on_string_with_one_comma():\u001b[0m\n",
            "\u001b[1m        test_argument = \"2,081\"\u001b[0m\n",
            "\u001b[1m        expected = 2080\u001b[0m\n",
            "\u001b[1m        actual = convert_to_int(test_argument)\u001b[0m\n",
            "\u001b[1m        # Format the string with the actual return value\u001b[0m\n",
            "\u001b[1m        message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\u001b[0m\n",
            "\u001b[1m        # Write the assert statement which prints message on failure\u001b[0m\n",
            "\u001b[1m>       assert actual == expected, message\u001b[0m\n",
            "\u001b[1m\u001b[31mE       AssertionError: convert_to_int('2,081') should return the int 2081, but it actually returned 2081\u001b[0m\n",
            "\u001b[1m\u001b[31mE       assert 2081 == 2080\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:13: AssertionError\n",
            "\u001b[1m\u001b[31m=========================== 1 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGq2hxS4Ws5U"
      },
      "source": [
        "# Use pytest.approx() in the assert statement\n",
        "# Note that 0.1+0.1+0.1 == 0.3 is FALSE.\n",
        "# Must use 0.1+0.1+0.1 == pytest.approx(0.3) to return TRUE.\n",
        "\n",
        "import numpy as np\n",
        "import pytest\n",
        "from as_numpy import get_data_as_numpy_array\n",
        "\n",
        "def test_on_clean_file():\n",
        "    expected = np.array([[2081.0, 314942.0],\n",
        "                        [1059.0, 186606.0],\n",
        "                        [1148.0, 206186.0]])\n",
        "    actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
        "    message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
        "    # Complete the assert statement\n",
        "    assert actual == pytest.approx(expected), message"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdoJNLC5JNnz"
      },
      "source": [
        "# Write multiple assert statements\n",
        "# This test will pass only if both assertions pass. \n",
        "# It will fail if any one of them raises an AssertionError.\n",
        "\n",
        "def test_on_six_rows():\n",
        "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
        "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
        "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
        "                                )\n",
        "    # Fill in with training array's expected number of rows\n",
        "    expected_training_array_num_rows = 4\n",
        "    # Fill in with testing array's expected number of rows\n",
        "    expected_testing_array_num_rows = 2\n",
        "    actual = split_into_training_and_testing_sets(example_argument)\n",
        "    # Write the assert statement checking training array's number of rows\n",
        "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
        "    # Write the assert statement checking testing array's number of rows\n",
        "    assert actual[1].shape[0] == expected_testing_array_num_rows, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMrat6E0Qh-t"
      },
      "source": [
        "### Test for exceptions\n",
        "\n",
        "import pytest\n",
        "\n",
        "# Fill in with a context manager that will silence the ValueError\n",
        "with pytest.raises(ValueError):\n",
        "    raise ValueError"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J28lUsmKTFwH",
        "outputId": "ac132c21-f42d-4119-9ea9-0ca483094124"
      },
      "source": [
        "import pytest\n",
        "\n",
        "try:\n",
        "    # Fill in with a context manager that raises Failed if no OSError is raised\n",
        "    with pytest.raises(OSError):\n",
        "        raise ValueError\n",
        "except:\n",
        "    print(\"pytest raised an exception because no OSError was raised in the context.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytest raised an exception because no OSError was raised in the context.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ-2GkgnTeI4"
      },
      "source": [
        "import pytest\n",
        "\n",
        "with pytest.raises(ValueError) as exc_info:\n",
        "    raise ValueError(\"Silence me!\")\n",
        "# Check if the raised ValueError contains the correct message\n",
        "assert exc_info.match(\"Silence me!\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9X6PYE1UAk9",
        "outputId": "3a01cb72-bf76-4b58-a5a1-575f770a0ff3"
      },
      "source": [
        "# %%file test_split_into_training_and_testing_sets.py\n",
        "# Unit test a ValueError\n",
        "# If the argument array has only 1 row, \n",
        "# the testing array will be empty. \n",
        "# To avoid this situation, you want the function to not return anything, \n",
        "# but raise a ValueError with the message \"Argument data_array must have at least 2 rows, \n",
        "# it actually has just 1\"\n",
        "\n",
        "import numpy as np\n",
        "import pytest\n",
        "from train import split_into_training_and_testing_sets\n",
        "\n",
        "def test_on_one_row():\n",
        "    test_argument = np.array([[1382.0, 390167.0]])\n",
        "    # Store information about raised ValueError in exc_info\n",
        "    with pytest.raises(ValueError) as exc_info:\n",
        "        split_into_training_and_testing_sets(test_argument)\n",
        "    expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
        "    # Check if the raised ValueError contains the correct message\n",
        "    assert exc_info.match(expected_error_msg)\n",
        "\n",
        "!pytest test_split_into_training_and_testing_sets.py"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_split_into_training_and_testing_sets.py .\u001b[36m                           [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 1 passed in 0.59 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "qeiW3hCaVfXY",
        "outputId": "8e220e2e-f556-4bf0-9b13-20790ac38dc1"
      },
      "source": [
        "test_argument = np.array([[1382.0, 390167.0]])\n",
        "split_into_training_and_testing_sets(test_argument)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-907bda4bd44e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1382.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m390167.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplit_into_training_and_testing_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/train.py\u001b[0m in \u001b[0;36msplit_into_training_and_testing_sets\u001b[0;34m(data_array)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Argument data_array must have at least 2 rows, it actually has just {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnum_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpermuted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Argument data_array must have at least 2 rows, it actually has just 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7uV0x6VzPE",
        "outputId": "4de438c3-08ce-443d-8f49-0ab6ea931731"
      },
      "source": [
        "# %%file test_row_to_list.py\n",
        "# How many tests for pytest\n",
        "# Check on bad arguments, special arguments, and normal arguments\n",
        "\n",
        "import pytest\n",
        "from preprocessing_helpers import row_to_list\n",
        "\n",
        "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
        "    # Assign actual to the return value for the argument \"123\\n\"\n",
        "    actual = row_to_list(\"123\\n\")\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
        "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
        "    # Complete the assert statement\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
        "    actual = row_to_list(\"\\t4,567\\n\")\n",
        "    # Format the failure message\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "\n",
        "def test_on_no_tab_with_missing_value():    # (0, 1) case\n",
        "    # Assign to the actual return value for the argument \"\\n\"\n",
        "    actual = row_to_list(\"\\n\")\n",
        "    # Write the assert statement with a failure message\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_on_two_tabs_with_missing_value():    # (2, 1) case\n",
        "    # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n",
        "    actual = row_to_list(\"123\\t\\t89\\n\")\n",
        "    # Write the assert statement with a failure message\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "\n",
        "def test_on_normal_argument_1():\n",
        "    actual = row_to_list(\"123\\t4,567\\n\")\n",
        "    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
        "    expected = [\"123\", \"4,567\"]\n",
        "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
        "    \n",
        "def test_on_normal_argument_2():\n",
        "    actual = row_to_list(\"1,059\\t186,606\\n\")\n",
        "    expected = [\"1,059\", \"186,606\"]\n",
        "    # Write the assert statement along with a failure message\n",
        "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
        "\n",
        "!pytest test_row_to_list.py\n",
        "\n",
        "# Note how mapping the arguments to tuples enabled us to categorize the arguments easily. \n",
        "# E.g. A row can be mapped to a 2-tuple (m, n), where m is the number of tab separators. n is 1 if the row has any missing values, and 0 otherwise.\n",
        "# \"123\\t456\\n\"  (1, 0)\n",
        "# \"\\t456\\n\"  (1, 1)\n",
        "# \"\\t456\\t\\n\"  (2, 1)\n",
        "# Use this trick for other functions whenever applicable ;-)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 7 items                                                             \u001b[0m\u001b[1m\rcollected 7 items                                                              \u001b[0m\n",
            "\n",
            "test_row_to_list.py .......\u001b[36m                                              [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 7 passed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_4EwTsSgLC1",
        "outputId": "7559fc53-e387-4cd9-fe3d-acaa98673bfa"
      },
      "source": [
        "# Test Driven Development (Pytest first, implement later)\n",
        "\n",
        "# Write three tests for normal arguments of convert_to_int()\n",
        "\n",
        "import pytest\n",
        "\n",
        "def test_with_no_comma():\n",
        "    actual = convert_to_int(\"756\")\n",
        "    # Complete the assert statement\n",
        "    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_with_one_comma():\n",
        "    actual = convert_to_int(\"2,081\")\n",
        "    # Complete the assert statement\n",
        "    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_with_two_commas():\n",
        "    actual = convert_to_int(\"1,034,891\")\n",
        "    # Complete the assert statement\n",
        "    assert actual == 1034891, \"Expected: 1034891, Actual: {0}\".format(actual)\n",
        "\n",
        "# Special arguments\n",
        "\n",
        "# Give a name to the test for an argument with missing comma\n",
        "def test_on_string_with_missing_comma():\n",
        "    actual = convert_to_int(\"178100,301\")\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_on_string_with_incorrectly_placed_comma():\n",
        "    # Assign to the actual return value for the argument \"12,72,891\"\n",
        "    actual = convert_to_int(\"12,72,891\")\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_on_float_valued_string():\n",
        "    actual = convert_to_int(\"23,816.92\")\n",
        "    # Complete the assert statement\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "\n",
        "!pytest test_convert_to_int.py\n",
        "\n",
        "# All tests below show errors because\n",
        "# convert_to_int() has not been implemented yet.\n",
        "# In TDD, the first run of the tests always fails with a NameError or ImportError \n",
        "# Because the function does not exist yet.\n",
        "# Note how thinking about special and bad arguments crystallized the requirements for the function. \n",
        "# This will help us immensely in implementing the function"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 6 items                                                             \u001b[0m\u001b[1m\rcollected 6 items                                                              \u001b[0m\n",
            "\n",
            "test_convert_to_int.py FFFFFF\u001b[36m                                            [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[1m\u001b[31m______________________________ test_with_no_comma ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_with_no_comma():\u001b[0m\n",
            "\u001b[1m>       actual = convert_to_int(\"756\")\u001b[0m\n",
            "\u001b[1m\u001b[31mE       NameError: global name 'convert_to_int' is not defined\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:4: NameError\n",
            "\u001b[1m\u001b[31m_____________________________ test_with_one_comma ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_with_one_comma():\u001b[0m\n",
            "\u001b[1m>       actual = convert_to_int(\"2,081\")\u001b[0m\n",
            "\u001b[1m\u001b[31mE       NameError: global name 'convert_to_int' is not defined\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:9: NameError\n",
            "\u001b[1m\u001b[31m_____________________________ test_with_two_commas _____________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_with_two_commas():\u001b[0m\n",
            "\u001b[1m>       actual = convert_to_int(\"1,034,891\")\u001b[0m\n",
            "\u001b[1m\u001b[31mE       NameError: global name 'convert_to_int' is not defined\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:14: NameError\n",
            "\u001b[1m\u001b[31m______________________ test_on_string_with_missing_comma _______________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_on_string_with_missing_comma():\u001b[0m\n",
            "\u001b[1m>       actual = convert_to_int(\"178100,301\")\u001b[0m\n",
            "\u001b[1m\u001b[31mE       NameError: global name 'convert_to_int' is not defined\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:22: NameError\n",
            "\u001b[1m\u001b[31m_________________ test_on_string_with_incorrectly_placed_comma _________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_on_string_with_incorrectly_placed_comma():\u001b[0m\n",
            "\u001b[1m        # Assign to the actual return value for the argument \"12,72,891\"\u001b[0m\n",
            "\u001b[1m>       actual = convert_to_int(\"12,72,891\")\u001b[0m\n",
            "\u001b[1m\u001b[31mE       NameError: global name 'convert_to_int' is not defined\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:27: NameError\n",
            "\u001b[1m\u001b[31m_________________________ test_on_float_valued_string __________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_on_float_valued_string():\u001b[0m\n",
            "\u001b[1m>       actual = convert_to_int(\"23,816.92\")\u001b[0m\n",
            "\u001b[1m\u001b[31mE       NameError: global name 'convert_to_int' is not defined\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_convert_to_int.py\u001b[0m:31: NameError\n",
            "\u001b[1m\u001b[31m=========================== 6 failed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEtl0IdpB5jI",
        "outputId": "145593bb-b662-4dcd-8766-d6b0c080fcd9"
      },
      "source": [
        "import pytest\n",
        "\n",
        "def test_with_no_comma():\n",
        "    actual = convert_to_int(\"756\")\n",
        "    # Complete the assert statement\n",
        "    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_with_one_comma():\n",
        "    actual = convert_to_int(\"2,081\")\n",
        "    # Complete the assert statement\n",
        "    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_with_two_commas():\n",
        "    actual = convert_to_int(\"1,034,891\")\n",
        "    # Complete the assert statement\n",
        "    assert actual == 1034891, \"Expected: 1034891, Actual: {0}\".format(actual)\n",
        "\n",
        "# Special arguments\n",
        "\n",
        "# Give a name to the test for an argument with missing comma\n",
        "def test_on_string_with_missing_comma():\n",
        "    actual = convert_to_int(\"178100,301\")\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_on_string_with_incorrectly_placed_comma():\n",
        "    # Assign to the actual return value for the argument \"12,72,891\"\n",
        "    actual = convert_to_int(\"12,72,891\")\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "    \n",
        "def test_on_float_valued_string():\n",
        "    actual = convert_to_int(\"23,816.92\")\n",
        "    # Complete the assert statement\n",
        "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
        "\n",
        "def convert_to_int(integer_string_with_commas):\n",
        "    comma_separated_parts = integer_string_with_commas.split(\",\")\n",
        "    for i in range(len(comma_separated_parts)):\n",
        "        # Write an if statement for checking missing commas\n",
        "        if len(comma_separated_parts[i]) > 3:\n",
        "            return None\n",
        "        # Write the if statement for incorrectly placed commas\n",
        "        if i != 0 and len(comma_separated_parts[i]) != 3:\n",
        "            return None\n",
        "    integer_string_without_commas = \"\".join(comma_separated_parts)\n",
        "    try:\n",
        "        return int(integer_string_without_commas)\n",
        "    # Fill in with a ValueError\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "!pytest test_convert_to_int.py\n",
        "\n",
        "# After writing the function that accounts for all cases in the testing function,\n",
        "# the pytest pass all tests."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 6 items                                                             \u001b[0m\u001b[1m\rcollected 6 items                                                              \u001b[0m\n",
            "\n",
            "test_convert_to_int.py ......\u001b[36m                                            [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 6 passed in 0.03 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSE7l-5eFCeY"
      },
      "source": [
        "# Test class\n",
        "\n",
        "import pytest\n",
        "import numpy as np\n",
        "\n",
        "from train import split_into_training_and_testing_sets\n",
        "\n",
        "# Declare the test class\n",
        "class TestSplitIntoTrainingAndTestingSets(object):\n",
        "    # Fill in with the correct mandatory argument\n",
        "    def test_on_one_row(self):\n",
        "        test_argument = np.array([[1382.0, 390167.0]])\n",
        "        with pytest.raises(ValueError) as exc_info:\n",
        "            split_into_training_and_testing_sets(test_argument)\n",
        "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
        "        assert exc_info.match(expected_error_msg)\n",
        "\n",
        "# เปรียบเทียบกับฟังก์ชันที่ไม่ได้เขียนเป็นคลาส\n",
        "\n",
        "def test_on_one_row():\n",
        "    test_argument = np.array([[1382.0, 390167.0]])\n",
        "    with pytest.raises(ValueError) as exc_info:\n",
        "        split_into_training_and_testing_sets(test_argument)\n",
        "    expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
        "    assert exc_info.match(expected_error_msg)\n",
        "\n",
        "# จะเห็นว่าจุดแตกต่างมีเพียงแค่เขียนชื่อ class เพื่อ test ฟังก์ชันนั้น โดยมี \"object\" เป็นตัวแปร\n",
        "# และตัวฟังก์ชันใน class นั้น จากเดิมที่เคยเขียนตัวแปรว่างไว้ ให้ใส่ self แทน"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Ur0Z32QdGu"
      },
      "source": [
        "# The current working directory in the IPython console is the tests folder that contains all tests. \n",
        "# The test class TestSplitIntoTrainingAndTestingSets resides in the test module tests/models/test_train.py\n",
        "\n",
        "# พิมพ์ pip install -e . ใน Anaconda prompt ใน directory ที่มีไฟล์ setup.py จึงจะทำได้\n",
        "# แต่ทำบน colab ไม่ได้ ไม่รู้ทำไม\n",
        "# ถ้าทำได้แล้วครั้งนึง จะ run pytest จาก ipython console ได้ตลอด \n",
        "# ไม่แน่ใจว่าทำจาก jupyter notebook จะได้ไหม\n",
        "\n",
        "# The command to run all the tests in this test class using node IDs\n",
        "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets\n",
        "\n",
        "# The command to run only  test test_on_six_rows() using node IDs\n",
        "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows\n",
        "\n",
        "# The command to run the tests in TestSplitIntoTrainingAndTestingSets using keyword expression\n",
        "!pytest -k \"SplitInto\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7gAtS45LVpR",
        "outputId": "75ab7d27-df5c-4523-adde-2c197d8ae503"
      },
      "source": [
        "!pip install -e ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbZBLYJ1jKLJ"
      },
      "source": [
        "# Add a decorator to make this function a fixture\n",
        "@pytest.fixture\n",
        "def clean_data_file():\n",
        "    file_path = \"clean_data_file.txt\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
        "    yield file_path\n",
        "    os.remove(file_path)\n",
        "    \n",
        "# Pass the correct argument so that the test can use the fixture\n",
        "def test_on_clean_file(clean_data_file):\n",
        "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
        "    # Pass the clean data file path yielded by the fixture as the first argument\n",
        "    actual = get_data_as_numpy_array(clean_data_file, 2)\n",
        "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryWhAR7nv31r",
        "outputId": "69e3797c-aa79-4bea-904b-975aa2468b6d"
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import pytest\n",
        "import numpy as np\n",
        "from as_numpy import get_data_as_numpy_array\n",
        "\n",
        "@pytest.fixture\n",
        "def empty_file():\n",
        "    # Assign the file path \"empty.txt\" to the variable\n",
        "    file_path = \"empty.txt\"\n",
        "    open(file_path, \"w\").close()\n",
        "    # Yield the variable file_path\n",
        "    yield file_path\n",
        "    # Remove the file in the teardown\n",
        "    os.remove(file_path)\n",
        "    \n",
        "def test_on_empty_file(self, empty_file):\n",
        "    expected = np.empty((0, 2))\n",
        "    actual = get_data_as_numpy_array(empty_file, 2)\n",
        "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
        "\n",
        "!pytest /content/test_as_numpy.py"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 1 item                                                              \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_as_numpy.py E\u001b[36m                                                       [100%]\u001b[0m\n",
            "\n",
            "==================================== ERRORS ====================================\n",
            "_____________________ ERROR at setup of test_on_empty_file _____________________\n",
            "file /content/test_as_numpy.py, line 17\n",
            "  def test_on_empty_file(self, empty_file):\n",
            "\u001b[31mE       fixture 'self' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, empty_file, monkeypatch, pytestconfig, record_property, record_xml_attribute, record_xml_property, recwarn, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test_as_numpy.py:17\n",
            "\u001b[1m\u001b[31m=========================== 1 error in 0.06 seconds ============================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbMhKBz8y8BA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMujpCGj1jI1"
      },
      "source": [
        "# Fixture chaining using tmpdir\n",
        "\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
        "def empty_file(tmpdir):\n",
        "    # Use the appropriate method to create an empty file in the temporary directory\n",
        "    file_path = tmpdir.join(\"empty.txt\")\n",
        "    open(file_path, \"w\").close()\n",
        "    yield file_path\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpeJPyML4VeY"
      },
      "source": [
        "# Mocking เริ่มงงแล้ว\n",
        "\n",
        "# Define a function convert_to_int_bug_free\n",
        "def convert_to_int_bug_free(comma_separated_integer_string):\n",
        "    # Assign to the dictionary holding the correct return values \n",
        "    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
        "    # Return the correct result using the dictionary return_values\n",
        "    return return_values[comma_separated_integer_string]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwVqEDDi45Ku"
      },
      "source": [
        "# Add the correct argument to use the mocking fixture in this test\n",
        "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
        "    raw_path, clean_path = raw_and_clean_data_file\n",
        "    # Replace the dependency with the bug-free mock\n",
        "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
        "                                       side_effect=convert_to_int_bug_free)\n",
        "    preprocess(raw_path, clean_path)\n",
        "    # Check if preprocess() called the dependency correctly\n",
        "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
        "    with open(clean_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    first_line = lines[0]\n",
        "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
        "    second_line = lines[1]\n",
        "    assert second_line == \"2002\\\\t333209\\\\n\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v14yJRob45nN",
        "outputId": "9546038e-f8a7-416c-81f1-86c6882c27cd"
      },
      "source": [
        "# Test on perfect fit\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pytest\n",
        "from train import model_test\n",
        "from math import sin, cos, pi\n",
        "\n",
        "def test_on_perfect_fit():\n",
        "    # Assign to a NumPy array containing a linear testing set\n",
        "    # The question provided these numbers.\n",
        "    test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
        "\n",
        "    # Fill in with the expected value of r^2 in the case of perfect fit\n",
        "    expected = 1.0\n",
        "\n",
        "    # Fill in with the slope and intercept of the model\n",
        "    actual = model_test(test_argument, slope=2.0, intercept=1.0)\n",
        "\n",
        "    # Complete the assert statement\n",
        "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
        "\n",
        "# It is an art to figure out special testing sets where you can assert some equalities or inequalities.\n",
        "\n",
        "def test_on_circular_data(): # self argument is deleted for the sake of running on colab\n",
        "    theta = pi/4.0\n",
        "    # Assign to a NumPy array holding the circular testing data\n",
        "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
        "                              [0.0, 1.0],\n",
        "                              [cos(3 * theta), sin(3 * theta)],\n",
        "                              [-1.0, 0.0],\n",
        "                              [cos(5 * theta), sin(5 * theta)],\n",
        "                              [0.0, -1.0],\n",
        "                              [cos(7 * theta), sin(7 * theta)]]\n",
        "                             )\n",
        "    # Fill in with the slope and intercept of the straight line\n",
        "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
        "    # Complete the assert statement\n",
        "    assert actual == pytest.approx(0.0)\n",
        "\n",
        "!pytest test_train.py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "collected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_train.py ..\u001b[36m                                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=========================== 2 passed in 0.32 seconds ===========================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XTIQ1VSBbVl",
        "outputId": "6c8406cd-23ff-44db-f299-c4ebce86328b"
      },
      "source": [
        "# Test plot\n",
        "\n",
        "\n",
        "import pytest\n",
        "import numpy as np\n",
        "\n",
        "from plots import get_plot_for_best_fit_line\n",
        "\n",
        "class TestGetPlotForBestFitLine(object):\n",
        "    # Add the pytest marker which generates baselines and compares images\n",
        "    @pytest.mark.mpl_image_compare\n",
        "    def test_plot_for_almost_linear_data(self):\n",
        "        slope = 5.0\n",
        "        intercept = -2.0\n",
        "        x_array = np.array([1.0, 2.0, 3.0])\n",
        "        y_array = np.array([3.0, 8.0, 11.0])\n",
        "        title = \"Test plot for almost linear data\"\n",
        "        # Return the matplotlib figure returned by the function under test\n",
        "        return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)\n",
        "\n",
        "# This is what you actually run\n",
        "# !pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\"\n",
        "!pytest --mpl-generate-path /content/test_lots.py# -k \"test_plot_for_almost_linear_data\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
            "pytest: error: unrecognized arguments: --mpl-generate-path\n",
            "  inifile: None\n",
            "  rootdir: /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpQr_R0hDGrh",
        "outputId": "27c98d56-1e3c-4377-bb70-fa9f48db8d53"
      },
      "source": [
        "pytest -k \"TestGetPlotForBestFitLine\" --mpl"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biSJSdP_QDaG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x_array, y_array, \".\")\n",
        "    ax.plot([0, np.max(x_array)], [intercept, slope * np.max(x_array) + intercept], \"-\")\n",
        "    # Fill in with axis labels so that they match the baseline\n",
        "    ax.set(xlabel=\"area (square feet)\", ylabel=\"price (dollars)\", title=title)\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}